# ğŸ™ï¸ AI Twin Voice Bot

### **End-to-End Audio Intelligence â€¢ Ultra-Low Latency â€¢ Persona-Driven Agentic System**

A production-grade, fully asynchronous voice AI agent that mimics a specific persona (Parthiv S) to answer interview-style questions.
Engineered for **zero-budgeting**, **near-instant responses**, and **strict hallucination control**.

This bot combines a **Hybrid Memory Architecture**, **massive-context reasoning**, and **resilient audio streaming** to deliver a natural, real-time conversational experience.

---

## ğŸš€ Features & Engineering Highlights

### ğŸ”¥ 1. Hybrid Memory Architecture (Instant + Deep Reasoning)

The bot chooses between two thinking modes:

#### âš¡ **Fast Path (0ms latency) â€” RAM "Golden Answers"**

For predictable queries like:

* "Tell me about yourself"
* "What are your strengths?"

A semantic router instantly maps the intent â†’ RAM audio cache.

* No DB calls
* No LLM calls
* No TTS calls

ğŸ‘‰ **Pure instant audio playback.**

#### ğŸ§  **Slow Path (Deep Research Mode) â€” 120B Researcher Model**

When a question is complex or unique, the system invokes the massive-context researcher model and synthesizes a personalized answer using:

* `persona.json`
* `project_chunks.txt`
* 200k+ token context
* Strict identity enforcement

---

## ğŸ” 2. Defense-in-Depth Language Guardrails

Stops Spanish drift or hallucinations with:

* Forced-English Whisper STT
* `langdetect` validation
* Auto-correction via Llama-8B Translation/Fixer
* Summarizer with:

  * 4â€“5 sentences
  * 70â€“80 words
  * No preamble keywords
  * No incomplete sentences

This ensures **clean, English-only, controlled** voice output.

---

## ğŸ›ï¸ 3. Resiliency Cascades (Zero-Failure Audio)

* âœ” Multiple Groq API key failover
* âœ” Fallback to gTTS (Google TTS)
* âœ” Custom TokenTracker for TPM enforcement
* âœ” Handles Render cold starts gracefully

---



---

## ğŸ› ï¸ Tech Stack

### **Backend**

* Python 3.11
* FastAPI
* Uvicorn
* Async Groq API SDK

### **AI Models**

* Whisper Large V3
* Llama 3.1 8B (router)
* Llama 3.3 70B (summarizer)
* GPT-OSS-120B (researcher)
* PlayAI TTS

### **Storage**

* Supabase (PostgreSQL)
* Base64 encoded audio blobs
* RAM hydration at startup

### **Frontend**

* Vanilla JS
* HTML
* CSS Glassmorphism
* Streaming audio playback

### **Deployment**

* Render (Backend)
* GitHub Pages / Vercel (Frontend)

---

## âš™ï¸ Local Setup

### 1. Clone Repo
```bash
git clone https://github.com/parthivqw/voice-bot.git
cd voice-bot/backend
```

---

### 2. Environment Setup
```bash
python -m venv venv
venv\Scripts\activate   # Windows
pip install -r requirements.txt
```

---

### 3. Create `.env`
```
# Groq
GROQ_API_KEY_1=gsk_...
GROQ_API_KEY_2=gsk_...
GROQ_API_KEY_3=gsk_...

# Supabase
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=public_anon_key
SUPABASE_SERVICE_ROLE_KEY=service_role_key
```

---

### 4. Seed Memory (Generate "Golden Audio")
```bash
python seeder.py
```

---

### 5. Run Backend Server
```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

You should see:
```
ğŸš€ Cache Hydrated: X items loaded into RAM.
```

---

## ğŸ§ª API Endpoints

### **POST /chat**

Audio â†’ Audio (stream)

### **POST /chat/text**

Text â†’ Audio (stream)

### **GET /health**

Render warm-up ping

---

## ğŸ§  Engineering Rationale

### âŒ Why Not RAG?

Render free tier = **512MB**
RAG with embeddings + FAISS = **1GB+ RAM**

â†’ Crashes instantly
â†’ Switched to **Massive Context Prompting** (120B)

### âš¡ Why RAM Caching?

Supabase fetch latency = 400â€“800ms
RAM lookup = <1ms

â†’ 400Ã— faster responses

### ğŸ“ Why Header Hijack?

We embed text response in HTTP header:

`X-AI-Response-Text`

â†’ Enables typewriter effect **without WebSockets**

---

## ğŸ”® Future Roadmap

* [ ] HeyGen Video Avatar
* [ ] LangGraph multi-agent refactor
* [ ] Socket.IO interrupt support
* [ ] Telemetry dashboards

---

## ğŸ‘¨â€ğŸ’» Author

**Parthiv S**
AI/ML Engineer â€¢ Multi-Agent Systems â€¢ GenAI Specialist

---
